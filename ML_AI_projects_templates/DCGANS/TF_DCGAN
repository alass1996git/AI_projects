
print (disc_out)
​
TF_CE = tf.keras.losses.BinaryCrossentropy(from_logits=True)
​
# hybridized cross entropy loss
# we care that fakes are correctly classified and real outputs are correctly classified
def discriminator_loss(real_output, fake_output):
    real_loss = TF_CE(tf.ones_like(real_output), real_output)
    fake_loss = TF_CE(tf.zeros_like(fake_output), fake_output)
    total_loss = real_loss + fake_loss
    return total_loss
​
# we want the generator to trick the generator as much as possible
def generator_loss(fake_output):
    return TF_CE(tf.ones_like(fake_output), fake_output)
​
# initialize optimiizers
gen_opt = tf.keras.optimizers.Adam(1e-4)
disc_opt = tf.keras.optimizers.Adam(1e-4)
​
# compile a training set for the training procedure
@tf.function
def train_step(images):
    noise = tf.random.normal([BATCH_SIZE, noise_dim])
​
    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:
      generated_images = gen_m(noise, training=True)
​
      real_output = disc_m(images, training=True)
      fake_output = disc_m(generated_images, training=True)
​
      gen_loss = generator_loss(fake_output)
      disc_loss = discriminator_loss(real_output, fake_output)
​
    gradients_of_generator = gen_tape.gradient(gen_loss, gen_m.trainable_variables)
    gradients_of_discriminator = disc_tape.gradient(disc_loss, disc_m.trainable_variables)
​
    gen_opt.apply_gradients(zip(gradients_of_generator, gen_m.trainable_variables))
    disc_opt.apply_gradients(zip(gradients_of_discriminator, disc_m.trainable_variables))
    return gen_loss, disc_loss
    
def train(dataset, epochs):
    for epoch in range(25):
        start = time.time()
        print(epoch)
        i = 0
        for image_batch in dataset:
            dl, gl = train_step(image_batch)
            i = i + 1
            if i % 10 == 0:
                print(epoch, float(dl), float(gl))
​
​
train(train_dataset, EPOCHS)
​
fig = plt.figure(figsize=(4, 4))
​
test_input = tf.random.normal([num_examples_to_generate, noise_dim])
predictions = gen_m(test_input, training=False)
for i in range(predictions.shape[0]):
  plt.subplot(4, 4, i+1)
  plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')
  plt.axis('off')
plt.show()
        
​
